{
  "door_code": "A63AI_DEVELOPMENT_TOOLS",
  "semantic_path": "ARCHITECTURE.AI.DEVTOOLS",
  "aliases": [
    "AI tools",
    "inference frameworks",
    "ML frameworks",
    "AI development"
  ],
  "context_bundle": {
    "summary": "AI development tooling: inference frameworks, training libraries, deployment platforms, and the developer ecosystem.",
    "prerequisites": [
      "A50AI_LANDSCAPE_2025"
    ],
    "related_doors": [
      "A54LOCAL_INFERENCE",
      "A56TRAINING_EFFICIENCY"
    ],
    "onboarding": {
      "quick_start": "Training: PyTorch dominates, Hugging Face for models. Inference: vLLM (server), llama.cpp (local), TensorRT-LLM (NVIDIA). Ollama for easy local.",
      "full_context_path": "",
      "common_patterns": [
        "=== TRAINING FRAMEWORKS ===",
        "PYTORCH: De facto standard. Dynamic graphs. Research-friendly. Meta maintains.",
        "TENSORFLOW: Google. Static graphs. Production-focused. Losing mindshare to PyTorch.",
        "JAX: Google. Functional. Great for TPUs. Research labs love it.",
        "TREND: PyTorch dominant. Most new research uses PyTorch.",
        "=== MODEL HUBS ===",
        "HUGGING FACE: Primary model hub. 500K+ models. Transformers library.",
        "TRANSFORMERS: HF library. Unified API for most models. pip install transformers.",
        "DATASETS: HF dataset library. Easy access to training data.",
        "SPACES: HF demo hosting. Quick way to share model demos.",
        "=== INFERENCE SERVERS ===",
        "vLLM: High-throughput. PagedAttention for memory efficiency. Continuous batching.",
        "TensorRT-LLM: NVIDIA optimized. Best performance on NVIDIA hardware.",
        "TEXT-GENERATION-INFERENCE (TGI): HuggingFace's server. Good defaults.",
        "TRITON: NVIDIA inference server. Multi-model, multi-framework.",
        "=== LOCAL INFERENCE ===",
        "LLAMA.CPP: C++ inference. GGUF format. CPU and GPU. Runs everywhere.",
        "OLLAMA: Easy local. One-liner model downloads. 'ollama run llama3'.",
        "LM STUDIO: GUI for local models. Download, run, chat interface.",
        "EXLLAMAV2: Optimized for EXL2 quantization. Fast GPU inference.",
        "KOBOLDCPP: Fork of llama.cpp. Extra features for roleplay/stories.",
        "=== QUANTIZATION FORMATS ===",
        "GGUF: llama.cpp format. CPU/GPU. Most compatible. Various bit-widths (Q4, Q5, Q8).",
        "GPTQ: GPU-focused. 4-bit. Good quality. Requires calibration.",
        "AWQ: Activation-aware. Better quality than GPTQ for some models.",
        "EXL2: ExLlamaV2 format. Mixed precision. Fine-grained control.",
        "BNITSANDBYTES: Dynamic quantization. Easy to use with HF.",
        "=== APPLICATION FRAMEWORKS ===",
        "LANGCHAIN: LLM apps. Chains, agents, RAG. Most popular. Sometimes overengineered.",
        "LLAMAINDEX: RAG-focused. Good for document QA. Simpler than LangChain.",
        "HAYSTACK: NLP pipelines. Search, QA. Enterprise-friendly.",
        "SEMANTIC KERNEL: Microsoft. C#/.NET focused. Azure integration.",
        "=== RAG (Retrieval Augmented Generation) ===",
        "CONCEPT: Retrieve relevant docs, add to context, then generate.",
        "EMBEDDING: Convert text to vectors. Sentence-transformers common.",
        "VECTOR DB: Store embeddings. Chroma, Pinecone, Weaviate, Milvus.",
        "CHUNKING: Split docs into pieces. Size matters for retrieval quality.",
        "=== VECTOR DATABASES ===",
        "CHROMA: Simple. Local first. Good for prototyping.",
        "PINECONE: Cloud. Managed. Scales well. Paid.",
        "WEAVIATE: Open source. Hybrid search. Good features.",
        "MILVUS: Open source. Scales to billions. Production-grade.",
        "PGVECTOR: Postgres extension. Use your existing DB.",
        "=== EXPERIMENT TRACKING ===",
        "WEIGHTS & BIASES: Industry standard. Logging, visualization, model registry.",
        "MLFLOW: Open source. Experiment tracking, model serving.",
        "TENSORBOARD: Basic. Built into TensorFlow/PyTorch.",
        "=== CLOUD INFERENCE APIS ===",
        "TOGETHER: Multi-model API. Good pricing. DeepSeek available.",
        "REPLICATE: Run any model. Pay per second. Good for experimentation.",
        "MODAL: Serverless GPU. Python-native. Easy deployment.",
        "ANYSCALE: Ray-based. Good for distributed workloads.",
        "FIREWORKS: Fast inference. Good pricing.",
        "=== FINE-TUNING TOOLS ===",
        "PEFT: Parameter-efficient fine-tuning. LoRA, QLoRA.",
        "AXOLOTL: Config-driven fine-tuning. Easy to use.",
        "UNSLOTH: Fast fine-tuning. 2x speedup claims.",
        "TORCHTUNE: PyTorch-native. Meta's fine-tuning library.",
        "=== PROMPT ENGINEERING ===",
        "LANGSMITH: Prompt debugging and tracing.",
        "PROMPTFLOW: Microsoft. Visual prompt engineering.",
        "GUIDANCE: Structured generation. Constrain outputs.",
        "=== EVALUATION ===",
        "LM-EVAL-HARNESS: Standard benchmarking. EleutherAI.",
        "HELM: Stanford. Comprehensive benchmarks.",
        "OPENAI EVALS: OpenAI's eval framework.",
        "=== PHIVECTOR RELEVANT ===",
        "OLLAMA: Easy local deployment for testing models.",
        "LLAMA.CPP: What we'd use for local inference on 5070 Ti.",
        "LANGCHAIN: Could use for RAG with PhiSHRI integration.",
        "CHROMA: Local vector DB if we add semantic search."
      ],
      "known_errors": [
        "Framework fragmentation - different tools for different use cases, no one-size-fits-all",
        "Format lock-in - GGUF not compatible with vLLM, EXL2 not compatible with GGUF",
        "Dependency hell - local tools require manual dependency management",
        "NVIDIA-only - TensorRT, ExLlama don't work on AMD. ROCm support varies.",
        "LangChain complexity - often overengineered for simple use cases",
        "RAG quality - chunking, embedding, retrieval all affect output. Many ways to fail.",
        "Cloud pricing - varies wildly. Compare before committing.",
        "Model compatibility - not all models work with all tools. Check before choosing."
      ]
    },
    "resources": {
      "docs": [],
      "code": [],
      "tests": [],
      "errors": []
    },
    "metadata": {
      "last_updated": "2025-12-20T01:08:17.0872058+11:00",
      "confidence": 1.0,
      "tags": [
        "devtools",
        "frameworks",
        "inference",
        "training",
        "deployment",
        "ecosystem"
      ],
      "category": "ARCHITECTURE",
      "subcategory": "AI",
      "version": "1.0.0",
      "agent_affinity": [
        "DC",
        "KALIC"
      ],
      "size_bytes": 6738,
      "token_estimate": 1685.0
    }
  }
}
