{
  "door_code": "A56TRAINING_EFFICIENCY",
  "semantic_path": "ARCHITECTURE.AI.TRAINING_EFFICIENCY",
  "aliases": [
    "efficient training",
    "FP8 training",
    "training optimization",
    "compute efficiency",
    "training costs"
  ],
  "context_bundle": {
    "summary": "Efficient LLM training techniques: FP8 precision, pipeline parallelism, data efficiency, and reducing compute requirements. How DeepSeek trained frontier model for $5.5M while others spent $100M+.",
    "prerequisites": [
      "A50AI_LANDSCAPE_2025"
    ],
    "related_doors": [
      "A51DEEPSEEK_ARCHITECTURE",
      "A52MOE_PATTERNS",
      "A57AI_ECONOMICS",
      "A61AI_HARDWARE"
    ],
    "onboarding": {
      "quick_start": "DeepSeek trained frontier model for $5.5M using: FP8 mixed precision (halves memory), DualPipe (overlaps compute/communication), MoE (only 37B active of 671B), auxiliary-loss-free balancing. Llama 3 cost 10x+ more. Key: co-design of algorithms, frameworks, and hardware. Constraint-driven innovation.",
      "full_context_path": "",
      "common_patterns": [
        "=== THE EFFICIENCY PUZZLE ===",
        "LLAMA 3 405B: 30.8M GPU hours on H100s. ~$500M+ total cost.",
        "DEEPSEEK V3 685B: 2.788M GPU hours on H800s. ~$5.5M final run.",
        "RATIO: 11x fewer GPU hours for larger, competitive model.",
        "HOW: Architectural innovation + training optimization + hardware adaptation.",
        "=== FP8 MIXED PRECISION ===",
        "STANDARD: FP16 or BF16 (16-bit floats). 2 bytes per value.",
        "FP8: 8-bit floats. 1 byte per value. Halves memory bandwidth.",
        "CHALLENGE: FP8 has limited range/precision. Training can be unstable.",
        "DEEPSEEK INNOVATION: First successful FP8 training at frontier scale.",
        "IMPLEMENTATION: Tile-wise grouping (1x128) for activations, block-wise (128x128) for weights.",
        "ACCUMULATION: Promote to higher precision (CUDA cores) every 128 elements for stability.",
        "RESULT: Near-FP16 quality with half the memory bandwidth. Doubles effective throughput.",
        "=== PIPELINE PARALLELISM ===",
        "CONCEPT: Split model layers across GPUs. Pipeline data through.",
        "STANDARD: Sequential stages. GPU idle while waiting for earlier stages.",
        "BUBBLE: Idle time = pipeline bubble. Reduces utilization.",
        "DUALPIPE (DeepSeek): Novel algorithm overlapping compute and communication.",
        "REARRANGEMENT: Reorder attention, all-to-all dispatch, MLP, combine operations.",
        "RESULT: Nearly full GPU utilization despite pipeline parallelism.",
        "=== EXPERT PARALLELISM ===",
        "CONCEPT: Distribute MoE experts across nodes.",
        "CHALLENGE: All-to-all communication for routing tokens to experts.",
        "H800 CONSTRAINT: 400GB/s interconnect vs 900GB/s on H100.",
        "DEEPSEEK: Custom communication optimization to overcome bandwidth limit.",
        "OVERLAP: Achieve near-full computation-communication overlap.",
        "=== PARALLELISM CONFIGURATION ===",
        "DEEPSEEK V3: 16-way Pipeline Parallelism (PP)",
        "DEEPSEEK V3: 64-way Expert Parallelism (EP) across 8 nodes",
        "DEEPSEEK V3: ZeRO-1 Data Parallelism (DP)",
        "TOTAL: 2048 H800 GPUs in coordinated training.",
        "=== DATA EFFICIENCY ===",
        "LESS DATA: 14.8T tokens vs 15T+ for competitors.",
        "QUALITY FOCUS: High-quality data selection over raw volume.",
        "MTP BENEFIT: Multi-Token Prediction gives denser gradient signal per sample.",
        "CURRICULUM: Effective data ordering can improve sample efficiency.",
        "=== AUXILIARY-LOSS-FREE BALANCING ===",
        "PROBLEM: MoE expert load balancing traditionally uses auxiliary loss.",
        "AUXILIARY LOSS: Penalizes uneven expert usage. Hurts main task performance.",
        "DEEPSEEK: Dynamic bias term per expert instead of aux loss.",
        "MECHANISM: Underutilized expert → increase bias. Overutilized → decrease.",
        "RESULT: Better balance without degrading model quality.",
        "=== ZERO REDUNDANCY (ZeRO) ===",
        "ZeRO-1: Shard optimizer states across GPUs. Reduce memory per GPU.",
        "ZeRO-2: Also shard gradients.",
        "ZeRO-3: Also shard parameters. Maximum memory efficiency.",
        "DEEPSEEK: Uses ZeRO-1 for data parallelism component.",
        "=== GRADIENT CHECKPOINTING ===",
        "CONCEPT: Don't store all activations. Recompute during backward pass.",
        "TRADEOFF: Memory savings vs compute overhead (~30% more compute).",
        "USE CASE: When memory-constrained. Trade compute for memory.",
        "DEEPSEEK: Used selectively where memory is tight.",
        "=== SCALING LAWS ===",
        "CHINCHILLA: Optimal token count ≈ 20x parameter count.",
        "DEEPSEEK: 14.8T tokens / 671B params ≈ 22x. Slightly overtrained.",
        "DIMINISHING RETURNS: More tokens help but with decreasing benefit.",
        "EFFICIENCY: Better to train right-sized model than overtrain small one.",
        "=== H800 vs H100 ===",
        "SAME COMPUTE: Both have ~2000 TFLOPS at BF16, ~4000 at FP8.",
        "DIFFERENT BANDWIDTH: H800 = 400GB/s interconnect. H100 = 900GB/s.",
        "IMPLICATION: Communication-heavy workloads (MoE) bottleneck on H800.",
        "DEEPSEEK RESPONSE: Custom communication algorithms to compensate.",
        "IRONY: Sanctions-driven constraints led to valuable innovations.",
        "=== TRAINING STABILITY ===",
        "FP8 RISK: Lower precision can cause instability, NaN gradients.",
        "MoE RISK: Expert routing can collapse or oscillate.",
        "MITIGATION: Careful hyperparameter tuning, monitoring, checkpointing.",
        "DEEPSEEK: Extensive ablation studies to find stable configurations.",
        "=== COST BREAKDOWN ===",
        "PRE-TRAINING: 2,664K GPU hours (95% of total)",
        "CONTEXT EXTENSION: 119K GPU hours",
        "POST-TRAINING: 5K GPU hours (SFT + RLHF)",
        "TOTAL: 2,788K GPU hours",
        "COST: At $2/GPU hour = $5.576M (final run only)",
        "=== WHAT'S NOT INCLUDED ===",
        "R&D: Architecture exploration, ablation studies, failed experiments.",
        "DATA: Acquisition, cleaning, curation, quality filtering.",
        "INFRASTRUCTURE: Hardware purchase/rental prior to final run.",
        "SALARIES: Engineering team, researchers, operations.",
        "TRUE COST: Likely 5-20x the $5.5M figure. Still efficient.",
        "=== LESSONS FOR SMALLER SCALE ===",
        "ARCHITECTURE: MoE and sparse attention applicable at smaller scale.",
        "QUANTIZATION: FP8 inference more accessible than FP8 training.",
        "DATA QUALITY: Quality over quantity applies at all scales.",
        "DISTILLATION: Train on larger model outputs. Cheaper than from scratch.",
        "=== PHIVECTOR IMPLICATIONS ===",
        "EFFICIENCY PHILOSOPHY: Do more with less. Same as PhiVector principles.",
        "CONSTRAINT INNOVATION: Limited resources drive creative solutions.",
        "ARCHITECTURE CHOICE: MoE pattern mirrors our agent specialization."
      ],
      "known_errors": [
        "FP8 HARDWARE: Requires H100/H800 tensor cores. Not backward compatible with older GPUs.",
        "PIPELINE BUBBLES: Even DualPipe has some bubble. Efficiency < 100%.",
        "COMMUNICATION OVERHEAD: Expert parallelism communication can still bottleneck.",
        "STABILITY: FP8 and MoE both add training complexity. More things can go wrong.",
        "COST CLAIMS: $5.5M is final run only. Exclude R&D, data, failed runs. True cost much higher.",
        "REPLICABILITY: Custom HAI-LLM framework not public. Can't directly replicate.",
        "HYPERPARAMETER SENSITIVITY: Efficient training requires careful tuning. Easy to get wrong."
      ]
    },
    "resources": {
      "docs": [
        "https://arxiv.org/html/2412.19437v1 (DeepSeek V3 - training details)",
        "https://arxiv.org/abs/2005.14165 (GPT-3 - scaling laws)",
        "https://arxiv.org/abs/2203.15556 (Chinchilla - compute-optimal training)",
        "https://arxiv.org/abs/2310.06825 (ZeRO++)"
      ],
      "code": [],
      "tests": [],
      "errors": []
    },
    "metadata": {
      "last_updated": "2025-12-19T10:30:00.000000000+00:00",
      "confidence": 0.95,
      "tags": [
        "training",
        "efficiency",
        "fp8",
        "parallelism",
        "compute",
        "optimization",
        "dualpipe",
        "zero"
      ],
      "category": "ARCHITECTURE",
      "subcategory": "AI",
      "version": "2.0.0",
      "agent_affinity": [
        "DC"
      ],
      "size_bytes": 8598,
      "token_estimate": 2150.0
    }
  }
}
