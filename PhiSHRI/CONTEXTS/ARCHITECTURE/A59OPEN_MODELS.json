{
  "door_code": "A59OPEN_MODELS",
  "semantic_path": "ARCHITECTURE.AI.OPEN_SOURCE",
  "aliases": [
    "open source LLM",
    "open models",
    "Llama",
    "Qwen",
    "Mistral",
    "open weights"
  ],
  "context_bundle": {
    "summary": "Open-source/open-weight LLMs December 2025: DeepSeek, Llama, Qwen, Mistral - capabilities, licensing, and local deployment. The models that run on YOUR hardware.",
    "prerequisites": [
      "A50AI_LANDSCAPE_2025"
    ],
    "related_doors": [
      "A51DEEPSEEK_ARCHITECTURE",
      "A54LOCAL_INFERENCE",
      "A58FRONTIER_MODELS"
    ],
    "onboarding": {
      "quick_start": "DeepSeek V3.2 (685B MoE, MIT license) now matches GPT-5 on benchmarks. Meta Llama 4 continues strong open tradition. Alibaba Qwen 2.5 excellent multilingual. Mistral efficient small models. Key: open weights enable local deployment, fine-tuning, and zero marginal cost inference.",
      "full_context_path": "",
      "common_patterns": [
        "=== OPEN-WEIGHT vs OPEN-SOURCE ===",
        "OPEN-WEIGHT: Model weights released. Can run and fine-tune.",
        "OPEN-SOURCE: Weights + training code + data released. Can reproduce.",
        "REALITY: Most 'open' models are open-weight only. Training recipe proprietary.",
        "STILL VALUABLE: Open weights enable local deployment, customization.",
        "=== DEEPSEEK ===",
        "V3.2: 685B MoE (37B active). Matches GPT-5 on benchmarks.",
        "V3.2-SPECIALE: Reasoning variant. Extended thinking. IMO gold medals.",
        "R1: Reasoning model built on V3. RLVR training.",
        "CODER: Code-specialized variants. Strong on coding tasks.",
        "LICENSE: MIT. Permissive. Commercial OK.",
        "CATCH: Cannot run locally (690GB). API or cloud deployment only.",
        "DISTILLATIONS: Smaller distilled versions emerging. Watch for 7B-70B range.",
        "=== META LLAMA ===",
        "LLAMA 4 MAVERICK: 17B-128E architecture. Strong general model.",
        "LLAMA 4 SCOUT: Variant optimized for speed.",
        "LLAMA 3.1 8B/70B/405B: Previous gen. Still excellent.",
        "LICENSE: Llama License. Commercial OK with restrictions.",
        "RESTRICTIONS: Cannot use to train competing models. Usage limits.",
        "LOCAL FRIENDLY: 8B and 70B run well on consumer/prosumer hardware.",
        "=== ALIBABA QWEN ===",
        "QWEN 2.5: Latest family. 0.5B to 72B+ sizes.",
        "STRENGTHS: Best multilingual. Strong Chinese. Good coding.",
        "QWEN 2.5 CODER: Specialized coding models.",
        "QWEN 2.5 MATH: Specialized math models.",
        "LICENSE: Apache 2.0 (most variants). Very permissive.",
        "LOCAL PICK: Qwen 2.5 7B and 14B excellent for 16GB VRAM.",
        "=== MISTRAL ===",
        "MISTRAL 7B v0.3: Efficient base model.",
        "MISTRAL SMALL 3.1: Latest small model.",
        "MISTRAL NEMO: Collaborative with NVIDIA.",
        "MIXTRAL: MoE variants (8x7B, 8x22B).",
        "STRENGTHS: Efficiency. Good performance per parameter.",
        "LICENSE: Apache 2.0. Commercial friendly.",
        "EUROPEAN: EU-based company. Regulatory alignment.",
        "=== OTHER NOTABLE MODELS ===",
        "PHI-3/PHI-4 (Microsoft): Small, efficient. 3.8B punches above weight.",
        "GEMMA 2 (Google): 2B-27B range. Good for edge deployment.",
        "YI (01.AI): Chinese lab. Strong models.",
        "INTERNLM: Chinese lab. Good multilingual.",
        "=== SIZE CATEGORIES ===",
        "TINY (1-3B): Phi-3 Mini, Gemma 2B. Run anywhere. Limited capability.",
        "SMALL (7-8B): Llama 3.1 8B, Qwen 2.5 7B, Mistral 7B. Sweet spot for local.",
        "MEDIUM (13-14B): Qwen 2.5 14B. Excellent local if VRAM allows.",
        "LARGE (30-34B): Qwen 2.5 32B. Needs 16GB+ at Q4.",
        "XL (70B): Llama 3.1 70B. Needs 48GB+ or heavy quantization.",
        "XXL (405B+): Llama 3.1 405B, DeepSeek 685B. Multi-GPU server only.",
        "=== LICENSING COMPARISON ===",
        "MIT (DeepSeek): Do anything. No restrictions.",
        "APACHE 2.0 (Qwen, Mistral): Do anything. Patent protection.",
        "LLAMA LICENSE: Commercial OK but restrictions on competing models.",
        "ALWAYS CHECK: Licenses can change between versions.",
        "=== FINE-TUNING OPPORTUNITIES ===",
        "LORA: Low-rank adaptation. Cheap, easy, effective.",
        "QLORA: Quantized LoRA. Even cheaper.",
        "FULL FINE-TUNE: All parameters. Expensive but most flexible.",
        "DOMAIN ADAPTATION: Tune for your specific use case.",
        "=== QUANTIZATION AVAILABILITY ===",
        "GGUF: Most models have community GGUF quantizations.",
        "GPTQ: Popular for GPU inference.",
        "AWQ: Growing availability.",
        "SOURCE: TheBloke on HuggingFace, official repos, community.",
        "=== COMMUNITY ECOSYSTEM ===",
        "HUGGING FACE: Primary model hub. Weights, docs, demos.",
        "LLAMA.CPP: Primary local inference engine.",
        "OLLAMA: Easy model management and running.",
        "REDDIT/DISCORD: r/LocalLLaMA, model-specific communities.",
        "=== CAPABILITY TIERS (LOCAL) ===",
        "TIER 1: Qwen 2.5 32B Q4, Llama 3.1 70B Q3 - Near-frontier local.",
        "TIER 2: Qwen 2.5 14B, Llama 3.1 8B - Great quality, easy to run.",
        "TIER 3: Mistral 7B, Phi-3 - Good for constrained hardware.",
        "TIER 4: Gemma 2B, Phi-3 Mini - Run anywhere, limited capability.",
        "=== RECOMMENDATIONS BY USE CASE ===",
        "GENERAL CHAT: Llama 3.1 8B or Qwen 2.5 7B/14B",
        "CODING: DeepSeek Coder 6.7B, Qwen 2.5 Coder 7B",
        "MULTILINGUAL: Qwen 2.5 (best), Llama 3.1 (good)",
        "REASONING: Qwen 2.5 14B/32B with good system prompt",
        "EDGE/MOBILE: Phi-3 Mini, Gemma 2B",
        "=== PHIVECTOR RELEVANCE ===",
        "LOCAL FIRST: Open models enable sovereign local deployment.",
        "NO VENDOR LOCK: Can switch models freely. No API dependency.",
        "COST: Zero marginal cost after hardware.",
        "PRIVACY: Data never leaves machine.",
        "CUSTOMIZATION: Fine-tune for specific PhiVector tasks if needed."
      ],
      "known_errors": [
        "OPEN-WEIGHT != OPEN-SOURCE: Training code/data usually not released. Can't reproduce.",
        "LICENSE VARIATIONS: Check specific version. Licenses change between releases.",
        "QUANTIZATION QUALITY: Not all quantizations equal. Test for your use case.",
        "COMMUNITY SUPPORT: Varies by model. Some better documented than others.",
        "LARGE MODEL BARRIER: 70B+ needs serious hardware. Not truly 'local' for most.",
        "DISTILLATION CONCERNS: Some models may have trained on closed model outputs. Legal grey area.",
        "UPDATE LAG: Community quantizations may lag official releases."
      ]
    },
    "resources": {
      "docs": [
        "https://huggingface.co/deepseek-ai",
        "https://huggingface.co/meta-llama",
        "https://huggingface.co/Qwen",
        "https://huggingface.co/mistralai",
        "https://huggingface.co/TheBloke (quantizations)"
      ],
      "code": [],
      "tests": [],
      "errors": []
    },
    "metadata": {
      "last_updated": "2025-12-19T11:15:00.000000000+00:00",
      "confidence": 0.95,
      "tags": [
        "open-source",
        "open-weight",
        "llama",
        "deepseek",
        "qwen",
        "mistral",
        "local",
        "huggingface"
      ],
      "category": "ARCHITECTURE",
      "subcategory": "AI",
      "version": "2.0.0",
      "agent_affinity": [
        "DC",
        "KALIC"
      ],
      "size_bytes": 7513,
      "token_estimate": 1879.0
    }
  }
}
