{
  "door_code": "A60REASONING_MODELS",
  "semantic_path": "ARCHITECTURE.AI.REASONING",
  "aliases": [
    "reasoning models",
    "thinking models",
    "o3",
    "R1",
    "extended thinking",
    "chain-of-thought"
  ],
  "context_bundle": {
    "summary": "Reasoning/thinking models: o3, DeepSeek R1, extended thinking - chain-of-thought, verification, and the compute-quality tradeoff. The new frontier of AI capability.",
    "prerequisites": [
      "A50AI_LANDSCAPE_2025",
      "A55AGENTIC_TRAINING"
    ],
    "related_doors": [
      "A51DEEPSEEK_ARCHITECTURE",
      "A58FRONTIER_MODELS",
      "A57AI_ECONOMICS"
    ],
    "onboarding": {
      "quick_start": "Reasoning models spend more inference compute to 'think' before answering. o3 and R1 lead the pack. o3 achieved ARC-AGI breakthrough but costs 100-600x more than GPT-4o. DeepSeek R1 uses RLVR (verifiable rewards). V3.2-Speciale won IMO/IOI gold medals. Key insight: quality scales with thinking time.",
      "full_context_path": "",
      "common_patterns": [
        "=== THE REASONING SHIFT ===",
        "OLD: Single forward pass. Answer immediately.",
        "NEW: Multiple reasoning steps. Think, verify, then answer.",
        "WHY: Complex problems benefit from step-by-step reasoning.",
        "ANALOGY: System 1 (fast, intuitive) vs System 2 (slow, deliberate) thinking.",
        "=== CHAIN-OF-THOUGHT (CoT) ===",
        "CONCEPT: Model outputs reasoning steps before final answer.",
        "PROMPTING: 'Let's think step by step' improves performance.",
        "TRAINING: Can train models to always produce reasoning traces.",
        "BENEFIT: Better accuracy on complex problems. Explainable process.",
        "=== OPENAI o3 ===",
        "ARCHITECTURE: Built on GPT-5 base with reasoning fine-tuning.",
        "MODES: Variable compute allocation. More thinking = better answers.",
        "ARC-AGI: Breakthrough on abstraction benchmark. ~88% at high compute.",
        "COST: High compute mode 100-600x more expensive than GPT-4o.",
        "o3-MINI: Faster, cheaper variant. Still strong reasoning.",
        "POSITIONING: Premium reasoning for complex tasks.",
        "=== DEEPSEEK R1 ===",
        "BASE: Built on DeepSeek V3 architecture.",
        "TRAINING: RLVR - Reinforcement Learning with Verifiable Rewards.",
        "VERIFIABLE: Train on tasks with checkable answers (math, code).",
        "OPEN WEIGHTS: Available for local deployment (but large).",
        "COST: Much cheaper than o3 at comparable quality.",
        "=== DEEPSEEK V3.2-SPECIALE ===",
        "VARIANT: High-compute reasoning version of V3.2.",
        "TRAINING: Extended RL with reduced length penalty.",
        "CAPABILITY: IMO 2025 gold medal. IOI 2025 gold medal.",
        "LIMITATION: Reasoning-only. Not suitable for general chat.",
        "LATENCY: Slow. Minutes for complex problems.",
        "=== RLVR (Reinforcement Learning with Verifiable Rewards) ===",
        "CONCEPT: Learn from tasks where answer correctness is checkable.",
        "DOMAINS: Math (can verify), code (can run), logic (can check).",
        "ADVANTAGE: Scalable. Don't need human labels for every example.",
        "LIMITATION: Only works for verifiable domains.",
        "=== GRPO (Group Relative Policy Optimization) ===",
        "CONCEPT: Compare multiple model outputs. Reward better ones.",
        "NO REWARD MODEL: Don't need separate model to score outputs.",
        "DEEPSEEK: Primary RL method for post-training.",
        "EFFICIENT: Less infrastructure than traditional RLHF.",
        "=== THINKING TRACES ===",
        "VISIBLE: Some models expose reasoning process.",
        "HIDDEN: Some hide reasoning, only show final answer.",
        "CONTROL: APIs may offer choice (thinking_mode parameter).",
        "COST: Visible traces cost more (more output tokens).",
        "=== TEST-TIME COMPUTE SCALING ===",
        "CONCEPT: Spend more compute at inference, not just training.",
        "TRADITIONAL: Model quality fixed after training.",
        "REASONING: Quality improves with more thinking time.",
        "TRADEOFF: Cost and latency vs quality.",
        "INSIGHT: Maybe we're not compute-limited on training, but on inference.",
        "=== QUALITY vs COST TRADEOFF ===",
        "SIMPLE TASKS: Reasoning overkill. Use fast model.",
        "COMPLEX TASKS: Reasoning worthwhile. Better accuracy.",
        "ROUTING: Ideally, route tasks to appropriate model/mode.",
        "EXAMPLE: Use o3-mini for quick queries, o3 full for hard problems.",
        "=== TASK TYPES THAT BENEFIT ===",
        "MATH: Multi-step proofs. Verification catches errors.",
        "CODING: Complex algorithms. Can reason about edge cases.",
        "LOGIC: Puzzles, formal reasoning. Step-by-step helps.",
        "PLANNING: Multi-step plans. Can verify feasibility.",
        "ANALYSIS: Complex documents. Structured reasoning helps.",
        "=== TASK TYPES THAT DON'T BENEFIT ===",
        "SIMPLE QA: Straightforward questions. Overkill.",
        "CREATIVE: Open-ended creativity. No 'correct' answer.",
        "RETRIEVAL: Looking up facts. No reasoning needed.",
        "TRANSLATION: Direct mapping. Reasoning adds latency.",
        "=== HYBRID THINKING MODES ===",
        "DEEPSEEK V3.2: Supports thinking and non-thinking modes.",
        "TOGGLE: Can enable/disable per request.",
        "AUTOMATIC: Some models auto-detect when to think deeply.",
        "IDEAL: Right amount of thinking for each task.",
        "=== VERIFICATION AND SELF-CORRECTION ===",
        "SELF-CHECK: Model reviews its own reasoning.",
        "ERROR DETECTION: Can catch mistakes in intermediate steps.",
        "BACKTRACKING: Reconsider approach if stuck.",
        "LIMITATION: Self-verification not foolproof. Can be confidently wrong.",
        "=== BENCHMARKS ===",
        "ARC-AGI: Abstraction and Reasoning Corpus. o3 breakthrough.",
        "MATH: Competition math problems.",
        "AIME: American Invitational Mathematics Examination.",
        "IMO: International Mathematical Olympiad.",
        "IOI: International Olympiad in Informatics.",
        "=== PHIVECTOR RELEVANCE ===",
        "TASK ROUTING: DC could route complex tasks to reasoning mode.",
        "COST AWARENESS: Use reasoning only when needed.",
        "LOCAL REASONING: R1 derivatives may enable local reasoning models.",
        "VERIFICATION: Multi-agent verification mirrors self-check pattern."
      ],
      "known_errors": [
        "COST EXPLOSION: Reasoning models 100-600x more expensive for complex tasks.",
        "NOT ALWAYS BETTER: Simple tasks don't benefit. Waste of compute.",
        "LATENCY: Thinking takes time. Not suitable for real-time applications.",
        "CONFIDENTLY WRONG: Self-verification doesn't guarantee correctness.",
        "BENCHMARK GAMING: Models may be optimized for specific benchmarks.",
        "OVERTHINKING: Can reason incorrectly and convince itself of wrong answer.",
        "VARIABLE QUALITY: Same model, same prompt can give different reasoning paths."
      ]
    },
    "resources": {
      "docs": [
        "https://openai.com/o3/",
        "https://arxiv.org/abs/2501.12948 (DeepSeek R1 paper)",
        "https://arcprize.org/ (ARC-AGI benchmark)"
      ],
      "code": [],
      "tests": [],
      "errors": []
    },
    "metadata": {
      "last_updated": "2025-12-19T11:30:00.000000000+00:00",
      "confidence": 0.95,
      "tags": [
        "reasoning",
        "thinking",
        "o3",
        "r1",
        "chain-of-thought",
        "verification",
        "grpo",
        "rlvr"
      ],
      "category": "ARCHITECTURE",
      "subcategory": "AI",
      "version": "2.0.0",
      "agent_affinity": [
        "DC"
      ],
      "size_bytes": 7764,
      "token_estimate": 1941.0
    }
  }
}
