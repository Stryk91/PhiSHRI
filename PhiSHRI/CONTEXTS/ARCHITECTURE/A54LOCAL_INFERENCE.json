{
  "door_code": "A54LOCAL_INFERENCE",
  "semantic_path": "ARCHITECTURE.AI.LOCAL_INFERENCE",
  "aliases": [
    "local LLM",
    "local inference",
    "run locally",
    "home inference",
    "consumer AI",
    "self-hosted LLM"
  ],
  "context_bundle": {
    "summary": "Running LLMs locally: hardware requirements, quantization, inference engines, and what's realistically possible on consumer hardware. Focus on RTX 5070 Ti (16GB VRAM) capabilities.",
    "prerequisites": [
      "A50AI_LANDSCAPE_2025"
    ],
    "related_doors": [
      "A51DEEPSEEK_ARCHITECTURE",
      "A61AI_HARDWARE",
      "A63AI_DEVELOPMENT_TOOLS",
      "A59OPEN_MODELS"
    ],
    "onboarding": {
      "quick_start": "VRAM is the constraint. Rule: FP16 needs ~2GB per 1B params. Q4 quantization = ~0.5GB per 1B. RTX 5070 Ti (16GB) = ~32B params at Q4, ~16B at Q8, ~8B at FP16. Best local models: Llama 3.1 8B, Qwen 2.5 14B/7B, Mistral 7B, Phi-3. Use llama.cpp/Ollama for easy local running.",
      "full_context_path": "",
      "common_patterns": [
        "=== MEMORY CALCULATION ===",
        "FP32: 4 bytes per parameter. 7B model = 28GB. Too big for most consumer GPUs.",
        "FP16/BF16: 2 bytes per parameter. 7B model = 14GB. Fits 16GB GPU.",
        "Q8 (8-bit): 1 byte per parameter. 7B model = 7GB. Comfortable on 16GB.",
        "Q4 (4-bit): 0.5 bytes per parameter. 7B model = 3.5GB. Easy on 16GB.",
        "FORMULA: Model size (GB) ≈ Parameters (B) × Bytes per param",
        "=== RTX 5070 Ti (16GB) CAPACITY ===",
        "FP16: Up to ~8B parameters (with context overhead)",
        "Q8: Up to ~14-16B parameters comfortably",
        "Q4: Up to ~28-32B parameters",
        "Q3/Q2: Up to ~40-50B parameters (quality degradation)",
        "PRACTICAL: 7B-14B models run great. 30B+ possible but tight.",
        "=== CONTEXT LENGTH OVERHEAD ===",
        "KV CACHE: Grows with context length. Must budget VRAM.",
        "RULE: 1-2GB overhead for 4K context. 4-8GB for 32K+ context.",
        "TRADEOFF: Longer context = less room for model = smaller model.",
        "EXAMPLE: 14B Q4 with 4K context fits. 14B Q4 with 32K context = tight.",
        "=== QUANTIZATION METHODS ===",
        "GGUF: llama.cpp format. Q2-Q8 levels. Most popular for local.",
        "GPTQ: GPU-optimized quantization. Good quality at Q4.",
        "AWQ: Activation-aware quantization. Slightly better quality than GPTQ.",
        "EXL2: ExLlamaV2 format. Fine-grained bits per weight. Flexible.",
        "BNIB: bitsandbytes int8/int4. Easy to use with transformers.",
        "=== QUALITY vs COMPRESSION ===",
        "Q8: Near-lossless. <1% benchmark degradation.",
        "Q6: Very good. ~1% degradation.",
        "Q5: Good balance. ~1-2% degradation.",
        "Q4: Standard compression. ~2-3% degradation. Usually acceptable.",
        "Q3: Noticeable degradation. ~5%+. Use if no other option.",
        "Q2: Significant degradation. Last resort.",
        "TASK SENSITIVITY: Math/code more sensitive to quantization than prose.",
        "=== INFERENCE ENGINES ===",
        "LLAMA.CPP: Gold standard for local. CPU+GPU support. GGUF format. Active development.",
        "OLLAMA: User-friendly wrapper around llama.cpp. One-liner installs.",
        "EXLLAMAV2: Fast GPU inference. EXL2 format. Best for NVIDIA GPUs.",
        "VLLM: Production server. PagedAttention. Overkill for single-user.",
        "TRANSFORMERS: HuggingFace library. Flexible but slower.",
        "TENSORRT-LLM: NVIDIA optimized. Best perf on NVIDIA, complex setup.",
        "=== LLAMA.CPP SPECIFICS ===",
        "FORMAT: GGUF (previously GGML). Single file, easy to manage.",
        "GPU OFFLOAD: -ngl flag controls layers on GPU. More = faster.",
        "FULL OFFLOAD: Put all layers on GPU for best speed.",
        "MIXED: Some layers CPU, some GPU. Enables bigger models, slower.",
        "QUANTIZATION NAMING: Q4_K_M = Q4 quantization, K-quant, Medium.",
        "=== OLLAMA WORKFLOW ===",
        "INSTALL: Single binary. Windows/Mac/Linux.",
        "RUN: 'ollama run llama3.1:8b' - downloads and runs.",
        "MODELS: Pre-quantized models in Ollama library.",
        "API: Local OpenAI-compatible API at localhost:11434.",
        "CUSTOM: Can import GGUF files for models not in library.",
        "=== RECOMMENDED LOCAL MODELS (Dec 2025) ===",
        "GENERAL CHAT: Llama 3.1 8B, Qwen 2.5 7B/14B, Mistral 7B v0.3",
        "CODING: DeepSeek Coder 6.7B, CodeLlama 7B/13B, Qwen 2.5 Coder",
        "REASONING: Qwen 2.5 14B, Llama 3.1 8B (with system prompt)",
        "SMALL/FAST: Phi-3 Mini (3.8B), Gemma 2 2B",
        "MULTILINGUAL: Qwen 2.5 (best), Llama 3.1 (good)",
        "=== MODEL RECOMMENDATIONS FOR 16GB ===",
        "EASY WIN: Llama 3.1 8B Q4 - 4.5GB, leaves room for context. Great all-rounder.",
        "BALANCED: Qwen 2.5 14B Q4 - 8GB, excellent quality, fits with 16K context.",
        "STRETCH: Qwen 2.5 32B Q4 - 16GB, tight but possible, limit context to 4K.",
        "AVOID: 70B+ models - even at Q2, won't fit or will be painfully slow.",
        "=== CPU INFERENCE ===",
        "WHEN: No GPU, GPU too small, or model too large for VRAM.",
        "SPEED: 10-50x slower than GPU. 1-5 tok/s vs 30-100 tok/s.",
        "RAM: System RAM instead of VRAM. 64GB RAM = big models possible.",
        "THREADS: Use all cores. llama.cpp -t flag for thread count.",
        "VIABLE FOR: Occasional use, batch processing, very large models.",
        "=== HYBRID CPU+GPU ===",
        "CONCEPT: Some layers on GPU, rest on CPU.",
        "BENEFIT: Run bigger models than GPU alone allows.",
        "COST: Slower than full GPU. CPU layers bottleneck.",
        "LLAMA.CPP: -ngl 20 puts 20 layers on GPU, rest on CPU.",
        "TUNE: Find sweet spot. More GPU layers = faster but needs more VRAM.",
        "=== SPECULATIVE DECODING ===",
        "CONCEPT: Small draft model proposes tokens, large model verifies.",
        "SPEEDUP: 2-3x possible with good draft model.",
        "REQUIREMENT: Draft model much smaller than main model.",
        "EXAMPLE: Phi-3 draft for Qwen 14B main. Phi-3 proposes, Qwen verifies.",
        "SUPPORT: llama.cpp, ExLlamaV2 support this.",
        "=== BATCH SIZE ===",
        "SERVER: Batch multiple requests. Higher throughput.",
        "LOCAL/SINGLE USER: Batch size 1. Different optimization.",
        "IMPLICATION: Server benchmarks don't apply to local single-user.",
        "=== PERFORMANCE EXPECTATIONS (5070 Ti) ===",
        "8B Q4: 60-100 tok/s output",
        "14B Q4: 40-60 tok/s output",
        "32B Q4: 15-30 tok/s output (VRAM-limited)",
        "PROMPT: First token slower (prefill). Then streaming.",
        "CONTEXT: Longer context = slower prefill.",
        "=== PRACTICAL SETUP (WINDOWS) ===",
        "1. Install Ollama or llama.cpp",
        "2. Download GGUF model from HuggingFace",
        "3. Run: ollama run model or ./llama-cli -m model.gguf",
        "4. Adjust -ngl for GPU layers, -c for context",
        "5. Integrate with your tools via API",
        "=== PHIVECTOR INTEGRATION ===",
        "API: Ollama provides OpenAI-compatible API. Easy integration.",
        "FALLBACK: Local model as fallback when API unavailable.",
        "COST: Zero marginal cost after hardware. Good for high-volume tasks.",
        "PRIVACY: Data never leaves machine. Fully sovereign.",
        "LATENCY: Local = low latency. No network round-trip."
      ],
      "known_errors": [
        "VRAM != RAM: GPU memory is the constraint, not system RAM. 64GB RAM doesn't help if GPU has 8GB.",
        "MoE MEMORY: MoE models need all params loaded even if only some active. 685B DeepSeek needs 690GB.",
        "CONTEXT EATS VRAM: KV cache grows with context. 32K context can consume 4-8GB alone.",
        "QUANTIZATION SENSITIVITY: Math and code tasks degrade more from quantization than chat.",
        "CPU INFERENCE SLOW: Workable for occasional use but not practical for interactive chat.",
        "GGUF COMPATIBILITY: Not all models have good GGUF quantizations. Check availability.",
        "DRIVER ISSUES: CUDA driver version must match llama.cpp build. Keep updated.",
        "AMD SUPPORT: ROCm improving but still behind CUDA. Some models don't work."
      ]
    },
    "resources": {
      "docs": [
        "https://github.com/ggerganov/llama.cpp",
        "https://ollama.ai",
        "https://huggingface.co/TheBloke (GGUF quantizations)",
        "https://github.com/turboderp/exllamav2"
      ],
      "code": [],
      "tests": [],
      "errors": []
    },
    "metadata": {
      "last_updated": "2025-12-19T10:00:00.000000000+00:00",
      "confidence": 0.95,
      "tags": [
        "local",
        "inference",
        "hardware",
        "quantization",
        "vram",
        "consumer",
        "llama.cpp",
        "ollama",
        "gguf"
      ],
      "category": "ARCHITECTURE",
      "subcategory": "AI",
      "version": "2.0.0",
      "agent_affinity": [
        "DC",
        "KALIC"
      ],
      "size_bytes": 9010,
      "token_estimate": 2253.0
    }
  }
}
