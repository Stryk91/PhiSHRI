{
  "door_code": "A61AI_HARDWARE",
  "semantic_path": "ARCHITECTURE.AI.HARDWARE",
  "aliases": [
    "AI hardware",
    "GPUs",
    "H100",
    "accelerators",
    "AI chips",
    "RTX",
    "NVIDIA"
  ],
  "context_bundle": {
    "summary": "AI hardware landscape: NVIDIA GPUs, AMD alternatives, TPUs, specialized accelerators, and the compute supply chain. What runs the models.",
    "prerequisites": [
      "A50AI_LANDSCAPE_2025"
    ],
    "related_doors": [
      "A54LOCAL_INFERENCE",
      "A56TRAINING_EFFICIENCY",
      "A57AI_ECONOMICS"
    ],
    "onboarding": {
      "quick_start": "NVIDIA dominates: H100/H200 (datacenter), RTX 50-series (consumer). H800 is sanctions-compliant H100 for China. AMD ROCm improving but lags CUDA. TPUs for Google workloads. Consumer sweet spot: RTX 5070 Ti (16GB, ~$750) or RTX 5080 (16GB, ~$1000). Memory is the constraint, not compute.",
      "full_context_path": "",
      "common_patterns": [
        "=== NVIDIA DATACENTER (HOPPER) ===",
        "H100 SXM: 80GB HBM3, 3.97 exaFLOPS FP8, 900GB/s NVLink. The gold standard.",
        "H100 PCIe: 80GB HBM3, lower interconnect. For PCIe slots.",
        "H800: H100 with reduced interconnect (400GB/s) for China export compliance.",
        "H200: 141GB HBM3e. More memory for larger models.",
        "PRICING: $25-40K per GPU. Months-long waitlists.",
        "=== NVIDIA DATACENTER (BLACKWELL) ===",
        "B100: Next gen after Hopper. Even more capable.",
        "B200: High-memory variant. 192GB+ expected.",
        "GB200: Grace-Blackwell superchip. CPU+GPU integrated.",
        "STATUS: Ramping production late 2024/2025.",
        "=== NVIDIA CONSUMER (ADA/BLACKWELL) ===",
        "RTX 5090: 32GB GDDR7. Consumer king. ~$2000. Overkill for most.",
        "RTX 5080: 16GB GDDR7. ~$1000. Excellent for serious local inference.",
        "RTX 5070 TI: 16GB GDDR7. ~$750. Sweet spot for local LLMs. YOUR CARD.",
        "RTX 5070: 12GB GDDR7. ~$550. Good entry point but memory-limited.",
        "RTX 4090: 24GB. Previous gen king. Still excellent if you have one.",
        "RTX 4080/4070: Capable but less VRAM than 50-series equivalents.",
        "=== AMD DATACENTER ===",
        "MI300X: 192GB HBM3. Competitive with H100 on paper.",
        "MI300A: APU variant with CPU+GPU.",
        "CHALLENGE: ROCm software ecosystem behind CUDA.",
        "IMPROVING: Major investment in ROCm. Catching up.",
        "PRICING: Competitive with NVIDIA. Better availability sometimes.",
        "=== AMD CONSUMER ===",
        "RX 7900 XTX: 24GB GDDR6. Good VRAM, ROCm support improving.",
        "RX 7900 XT: 20GB GDDR6. Decent option for ROCm users.",
        "CHALLENGE: llama.cpp works, but CUDA still faster and easier.",
        "RECOMMENDATION: If you already have AMD, use it. Buying new? NVIDIA easier.",
        "=== GOOGLE TPU ===",
        "TPU v5p: Latest generation. Optimized for TensorFlow/JAX.",
        "TPU v4: Previous gen. Still powerful.",
        "ACCESS: Google Cloud only. No purchase option.",
        "STRENGTH: Cost-effective for training at Google scale.",
        "LIMITATION: Proprietary. Only works with Google stack.",
        "=== APPLE SILICON ===",
        "M3 MAX: 128GB unified memory option. Can run larger models.",
        "M3 ULTRA: 192GB unified memory. Impressive for local inference.",
        "BENEFIT: Unified memory architecture. No GPU/CPU memory split.",
        "LIMITATION: Metal vs CUDA. Some models not optimized.",
        "mlx: Apple's ML framework. Growing support.",
        "=== INTEL ===",
        "GAUDI 3: AI accelerator. Some adoption in datacenters.",
        "ARC: Consumer GPUs. Limited AI ecosystem.",
        "STATUS: Trying to compete but limited success.",
        "=== KEY SPECIFICATIONS ===",
        "VRAM: Most important for local inference. Determines model size.",
        "MEMORY BANDWIDTH: Affects token generation speed.",
        "TFLOPS: Raw compute. Less important than memory for inference.",
        "INTERCONNECT: NVLink/NVSwitch for multi-GPU training.",
        "=== MEMORY IS THE CONSTRAINT ===",
        "MODEL SIZE: Params Ã— bytes per param must fit in VRAM.",
        "KV CACHE: Grows with context length. Must budget for this.",
        "BATCHING: Multiple requests need more memory.",
        "RULE: Buy as much VRAM as you can afford.",
        "=== MULTI-GPU CONSIDERATIONS ===",
        "CONSUMER: No NVLink. PCIe only. Slower multi-GPU.",
        "TENSOR PARALLELISM: Split model across GPUs. Needs fast interconnect.",
        "PIPELINE PARALLELISM: Pipeline layers across GPUs. More latency-tolerant.",
        "RECOMMENDATION: For local, single large GPU usually better than multiple small.",
        "=== POWER AND COOLING ===",
        "H100: ~700W TDP. Datacenter cooling required.",
        "RTX 5090: ~450W TDP. Serious PSU and cooling needed.",
        "RTX 5070 Ti: ~250W TDP. Reasonable for desktop.",
        "ELECTRICITY: AI workloads use significant power. Factor into cost.",
        "=== SUPPLY CHAIN ===",
        "H100: Multi-month waitlists. Allocation scarce.",
        "CHINA SANCTIONS: H800 created for compliance. Driving innovation.",
        "PRICING: Demand exceeds supply. Prices elevated.",
        "ALTERNATIVE SOURCES: AMD, cloud providers sometimes better availability.",
        "=== CLOUD vs BUY ===",
        "CLOUD: Pay per hour. No upfront cost. Flexible.",
        "BUY: High upfront. Zero marginal cost after. Full control.",
        "BREAK-EVEN: Heavy users (>8 hrs/day) often better buying.",
        "HYBRID: Own baseline capacity, cloud for burst.",
        "=== RTX 5070 Ti SPECIFICS (YOUR CARD) ===",
        "VRAM: 16GB GDDR7. Good for 7B-32B models at various quantizations.",
        "BANDWIDTH: ~500+ GB/s expected. Fast inference.",
        "COMPUTE: Blackwell architecture. Efficient inference.",
        "SWEET SPOT: 8B Q8, 14B Q4-Q6, 32B Q3-Q4 all viable.",
        "VALUE: Best price/VRAM ratio in current lineup.",
        "=== PHIVECTOR RELEVANCE ===",
        "RTX 5070 Ti: Your card. 16GB enables solid local inference.",
        "64GB RAM: System RAM for CPU offload if needed.",
        "990 PRO SSD: Fast model loading. Good QoL improvement.",
        "Z170 LIMITS: PCIe 3.0 limits bandwidth. Works but not optimal."
      ],
      "known_errors": [
        "VRAM IS KEY: TFLOPS don't matter if model doesn't fit in memory.",
        "AMD ECOSYSTEM: ROCm improving but CUDA still easier and better supported.",
        "MULTI-GPU CONSUMER: No NVLink means limited benefit for tensor parallelism.",
        "POWER COSTS: AI workloads expensive to run. Factor electricity into TCO.",
        "SUPPLY: H100 and high-end parts have long lead times.",
        "PCIE GEN: Older motherboards (Z170) limited to PCIe 3.0. Some bandwidth loss.",
        "THERMALS: High-end GPUs need good cooling. Throttling hurts performance."
      ]
    },
    "resources": {
      "docs": [
        "https://www.nvidia.com/en-us/data-center/h100/",
        "https://www.nvidia.com/en-us/geforce/graphics-cards/50-series/",
        "https://www.amd.com/en/products/accelerators/instinct/mi300.html",
        "https://cloud.google.com/tpu"
      ],
      "code": [],
      "tests": [],
      "errors": []
    },
    "metadata": {
      "last_updated": "2025-12-19T11:45:00.000000000+00:00",
      "confidence": 0.95,
      "tags": [
        "hardware",
        "gpu",
        "nvidia",
        "h100",
        "rtx",
        "amd",
        "tpu",
        "vram"
      ],
      "category": "ARCHITECTURE",
      "subcategory": "AI",
      "version": "2.0.0",
      "agent_affinity": [
        "DC"
      ],
      "size_bytes": 7658,
      "token_estimate": 1915.0
    }
  }
}
