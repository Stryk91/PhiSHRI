{
  "door_code": "A53SPARSE_ATTENTION",
  "semantic_path": "ARCHITECTURE.AI.SPARSE_ATTENTION",
  "aliases": [
    "sparse attention",
    "efficient attention",
    "DSA",
    "flash attention",
    "linear attention",
    "long context attention"
  ],
  "context_bundle": {
    "summary": "Sparse attention mechanisms for efficient long-context: DSA, flash attention, sliding window, and reducing O(L²) to O(L) or O(kL). Critical for 128K+ context windows without quadratic memory/compute explosion.",
    "prerequisites": [
      "A50AI_LANDSCAPE_2025"
    ],
    "related_doors": [
      "A51DEEPSEEK_ARCHITECTURE",
      "A52MOE_PATTERNS",
      "A54LOCAL_INFERENCE"
    ],
    "onboarding": {
      "quick_start": "Standard self-attention is O(L²) - 128K context = 16 billion ops per layer. Sparse attention reduces this. DSA (DeepSeek): lightning indexer picks relevant tokens, O(kL). Flash attention: not sparse but memory-efficient via fused kernels. Sliding window: attend only to recent tokens. All enable long context without explosion.",
      "full_context_path": "",
      "common_patterns": [
        "=== THE QUADRATIC PROBLEM ===",
        "SELF-ATTENTION: Every token attends to every other token. QK^T matrix is L×L.",
        "L=1K: 1 million attention scores per layer. Manageable.",
        "L=32K: 1 billion attention scores per layer. Getting expensive.",
        "L=128K: 16 billion attention scores per layer. Prohibitive without optimization.",
        "MEMORY: Must store L×L attention matrix (or compute on-the-fly). At FP16: 128K×128K×2 = 32GB per layer.",
        "COMPUTE: L² multiply-adds per attention head. Dominates at long context.",
        "=== SOLUTION CATEGORIES ===",
        "SPARSE PATTERNS: Only compute attention for subset of token pairs. O(kL) or O(L).",
        "MEMORY EFFICIENCY: Compute full attention but don't materialize full matrix. Flash attention.",
        "APPROXIMATION: Approximate attention with linear complexity. Quality tradeoff.",
        "HYBRID: Combine approaches for different heads or layers.",
        "=== DEEPSEEK SPARSE ATTENTION (DSA) ===",
        "INNOVATION: Two-phase attention with learned sparsity.",
        "PHASE 1 - LIGHTNING INDEXER: Small, cheap attention heads (low precision) scan all token pairs. Output: relevance scores.",
        "PHASE 2 - SELECTIVE ATTENTION: Keep top-k most relevant positions per query. Run full MLA on sparse set only.",
        "COMPLEXITY: O(L²) indexing (cheap) + O(kL) full attention = O(kL) effective (k << L).",
        "K VALUE: Typically 4096-8192. At L=128K, this is 3-6% of tokens.",
        "TRAINING: Lightning indexer trained with KL-divergence to match full attention distribution.",
        "RESULT: 50% inference cost reduction at long context. Quality maintained.",
        "=== FLASH ATTENTION ===",
        "NOT SPARSE: Computes full attention, just memory-efficiently.",
        "PROBLEM: Standard attention materializes L×L matrix. Huge memory.",
        "SOLUTION: Fused CUDA kernel. Tile computation. Never materialize full matrix.",
        "TILING: Process attention in blocks. Accumulate softmax incrementally.",
        "BENEFIT: O(L) memory instead of O(L²). Same exact computation, just smarter.",
        "VERSIONS: Flash Attention 1, 2, 3. Each faster. Widely adopted.",
        "LIMITATION: Still O(L²) compute. Just memory-efficient, not compute-efficient.",
        "=== SLIDING WINDOW ATTENTION ===",
        "CONCEPT: Each token attends only to last W tokens.",
        "COMPLEXITY: O(WL) where W is window size (e.g., 4096).",
        "LOSS: No direct attention to tokens beyond window. Must rely on information propagation.",
        "USE CASE: Streaming, very long documents where local context sufficient.",
        "MODELS: Mistral, some LongLoRA variants.",
        "=== GROUPED QUERY ATTENTION (GQA) ===",
        "CONCEPT: Share KV heads across multiple query heads.",
        "STANDARD MHA: 32 Q heads, 32 K heads, 32 V heads.",
        "GQA: 32 Q heads, 8 K heads, 8 V heads. K/V shared across query groups.",
        "BENEFIT: Smaller KV cache. Critical for inference memory.",
        "QUALITY: Minimal degradation with proper training.",
        "MODELS: Llama 2+, most modern models.",
        "=== MULTI-HEAD LATENT ATTENTION (MLA) ===",
        "DEEPSEEK INNOVATION: Compress KV before caching, decompress at attention time.",
        "COMPRESSION: Project K, V to lower dimension (latent space).",
        "CACHE: Store compressed KV. Much smaller than full KV.",
        "DECOMPRESSION: Project back to full dimension when computing attention.",
        "BENEFIT: Massive KV cache reduction. Enables longer context in same memory.",
        "COMBINATION: DSA + MLA = DeepSeek's full attention stack.",
        "=== LINEAR ATTENTION ===",
        "CONCEPT: Approximate softmax(QK^T)V with kernel methods.",
        "MECHANISM: φ(Q)φ(K)^T V can be computed as φ(Q)(φ(K)^T V) in O(L).",
        "QUALITY: Approximation loses some attention precision. Mixed results.",
        "MODELS: Linear Transformer, Performer, RWKV (different approach).",
        "STATUS: Not widely adopted for frontier models. Quality concerns.",
        "=== HIERARCHICAL ATTENTION ===",
        "CONCEPT: Coarse global attention + fine local attention.",
        "GLOBAL: Attend to summary tokens or strided positions. Captures long-range.",
        "LOCAL: Dense attention within windows. Captures detail.",
        "EXAMPLES: Longformer, BigBird, LED.",
        "TRADEOFF: Complexity vs. quality. Sparse patterns are hand-designed.",
        "=== LEARNED SPARSE PATTERNS ===",
        "CONCEPT: Let model learn which positions to attend to.",
        "DSA: Lightning indexer learns relevance scores.",
        "ROUTING ATTENTION: Route queries to relevant key subsets.",
        "ADVANTAGE: Adaptive to content, not fixed pattern.",
        "CHALLENGE: Learning sparse patterns adds training complexity.",
        "=== IMPLEMENTATION CONSIDERATIONS ===",
        "CUDA KERNELS: Sparse attention needs custom kernels. Not all patterns easy to implement.",
        "HARDWARE: Some sparse patterns don't map well to GPU architecture.",
        "FRAMEWORKS: Flash attention widely supported. Custom sparse patterns less so.",
        "QUANTIZATION: Sparse attention + quantization = complex interaction.",
        "=== CONTEXT LENGTH IMPLICATIONS ===",
        "1K-8K: Dense attention fine. Flash attention helps memory.",
        "8K-32K: Flash attention essential. Consider GQA for KV cache.",
        "32K-128K: Need sparse attention (DSA) or sliding window for efficiency.",
        "128K+: Full sparse stack required. DSA + MLA or similar.",
        "=== PHIVECTOR RELEVANCE ===",
        "PHISHRI PARALLEL: 'Lightning indexer' concept = cheap first-pass to identify relevant items.",
        "SEARCH PATTERN: Index all doors cheaply, then fetch only relevant ones. Same idea as DSA.",
        "FUTURE: As PhiSHRI grows, sparse retrieval patterns become more important.",
        "LOCAL INFERENCE: Sparse attention enables longer context on consumer hardware."
      ],
      "known_errors": [
        "DSA WARMUP: Benefits only appear at longer contexts (>32K). Overhead at short context.",
        "FLASH ATTENTION HARDWARE: Requires specific GPU architectures. Not all hardware supported.",
        "SPARSE PATTERNS MISS: Hand-designed patterns can miss important long-range dependencies.",
        "LINEAR ATTENTION QUALITY: Approximations often degrade quality. Not production-ready for frontier.",
        "SLIDING WINDOW LOSS: Information beyond window must propagate through layers. Can be lost.",
        "IMPLEMENTATION COMPLEXITY: Custom CUDA kernels required. Hard to modify/debug.",
        "TRAINING INSTABILITY: Some sparse patterns make training less stable.",
        "KV CACHE TRADEOFF: Compressed KV (MLA) requires decompression compute at inference."
      ]
    },
    "resources": {
      "docs": [
        "https://arxiv.org/abs/2512.02556 (DeepSeek Sparse Attention)",
        "https://arxiv.org/abs/2205.14135 (Flash Attention)",
        "https://arxiv.org/abs/2307.08691 (Flash Attention 2)",
        "https://arxiv.org/abs/2004.05150 (Longformer)",
        "https://arxiv.org/abs/2007.14062 (BigBird)"
      ],
      "code": [
        "https://github.com/Dao-AILab/flash-attention"
      ],
      "tests": [],
      "errors": []
    },
    "metadata": {
      "last_updated": "2025-12-19T09:45:00.000000000+00:00",
      "confidence": 0.95,
      "tags": [
        "attention",
        "sparse",
        "efficiency",
        "long-context",
        "dsa",
        "flash-attention",
        "mla",
        "gqa"
      ],
      "category": "ARCHITECTURE",
      "subcategory": "AI",
      "version": "2.0.0",
      "agent_affinity": [
        "DC"
      ],
      "size_bytes": 8858,
      "token_estimate": 2215.0
    }
  }
}
