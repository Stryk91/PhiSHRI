{
  "door_code": "W87ETL",
  "semantic_path": "WORKFLOWS.DATA_PROCESSING.ETL_PIPELINES",
  "aliases": [
    "etl",
    "extract",
    "transform",
    "load",
    "data_pipeline",
    "etl_process"
  ],
  "context_bundle": {
    "summary": "ETL (Extract, Transform, Load) pipelines extract data from sources, transform it into desired format, and load into target systems with validation, error handling, and monitoring at each stage.",
    "prerequisites": [
      "W84LOGGING",
      "W86METRICS",
      "E09VALIDATION_ERRORS"
    ],
    "related_doors": [
      "W88STREAMING",
      "W89DATA_VALIDATION",
      "W90BATCH_PROCESSING",
      "W91DATA_PIPELINE",
      "W92DATA_TRANSFORM",
      "T18AIRFLOW"
    ],
    "onboarding": {
      "quick_start": "ETL stages: 1) Extract: Read from sources (databases, APIs, files) with retry logic. 2) Transform: Clean, validate, enrich, aggregate data using business rules. 3) Load: Write to target (data warehouse, lake, database) with deduplication and error handling. Monitor each stage with metrics and logs.",
      "full_context_path": "/PhiDEX/industry_knowledge/data_processing_patterns.md",
      "common_patterns": [
        "Extract pattern: def extract_from_db(): conn = connect_db(); cursor = conn.cursor(); data = cursor.execute('SELECT * FROM source_table'); return [dict(row) for row in data.fetchall()]",
        "Transform pattern: def transform(records): return [{'id': r['id'], 'name': r['name'].upper(), 'created': parse_date(r['date']), 'amount': float(r['amount'])} for r in records if validate(r)]",
        "Load pattern: def load_to_warehouse(records): with warehouse.connection() as conn: conn.executemany('INSERT INTO target (id, name, created, amount) VALUES (?, ?, ?, ?)', [(r['id'], r['name'], r['created'], r['amount']) for r in records]); conn.commit()",
        "Error handling: try: extract() -> transform() -> load() except ExtractError: log_error('extraction_failed'); raise except TransformError: log_error('transformation_failed'); store_failed_records() except LoadError: log_error('load_failed'); rollback_transaction()"
      ],
      "known_errors": [
        "E09VALIDATION_ERRORS",
        "E07DATABASE_ERRORS",
        "E08NETWORK_ERRORS"
      ]
    },
    "resources": {
      "docs": [
        "/PhiDEX/industry_knowledge/data_processing_patterns.md"
      ],
      "code": [],
      "tests": [],
      "errors": [
        "E09VALIDATION_ERRORS",
        "E07DATABASE_ERRORS",
        "E08NETWORK_ERRORS"
      ]
    },
    "metadata": {
      "last_updated": "2025-11-21T00:00:00Z",
      "confidence": 1.0,
      "usage_count": 0,
      "success_rate": 0.0,
      "tags": [
        "etl",
        "data_pipeline",
        "extract",
        "transform",
        "load",
        "data_processing",
        "batch_processing"
      ],
      "category": "WORKFLOWS",
      "subcategory": "DATA_PROCESSING",
      "version": "1.0.0",
      "tested_on": [
        "Python",
        "Java",
        "Scala",
        "SQL",
        "Apache Spark",
        "AWS Glue"
      ],
      "agent_affinity": [
        "DC",
        "VSCC",
        "WEBC"
      ],
      "size_bytes": 2830,
      "token_estimate": 708.0
    }
  }
}
